!pip install transformers datasets accelerate rouge-score nltk evaluate

import torch
from transformers import (
    BartTokenizer,
    BartForConditionalGeneration,
    Seq2SeqTrainer,
    Seq2SeqTrainingArguments,
    DataCollatorForSeq2Seq
)
from datasets import Dataset
import evaluate
import numpy as np
import nltk
from nltk.translate.bleu_score import corpus_bleu, sentence_bleu
from rouge_score import rouge_scorer

nltk.download('punkt')

model_name = "facebook/bart-base"
tokenizer = BartTokenizer.from_pretrained(model_name)
model = BartForConditionalGeneration.from_pretrained(model_name)

inputs = [
    "Two roads diverged in a yellow wood,\nAnd sorry I could not travel both",
    "I shall be telling this with a sigh\nSomewhere ages and ages hence:",
    "Yet knowing how way leads on to way,\nI doubted if I should ever come back.",
    "And looked down one as far as I could\nTo where it bent in the undergrowth;",
    "The woods are lovely, dark and deep,\nBut I have promises to keep,",
    "The old dog barks backward without getting up. / I can remember when he was a pup.",
    "Something there is that doesn't love a wall, / That sends the frozen-ground-swell under it, / And spills the upper boulders in the sun.",
    "Earth's the right place for love: I don't know where it's likely to go better.",
    "The best way out is always through.",
    "So all by myself, without a fellow to tell me, / I sat and told it to the five-foot wall / That hadn't an ear.",
    "All out of doors looked darkly in at him",
    "Through the thin frost, almost in separate stars,",
    "That gathers on the pane in empty rooms.",
    "What kept his eyes from giving back the gaze",
    "Was the lamp tilted near them in his hand.",
    "What kept him from remembering what it was",
    "That brought him to that creaking room was age."
]

targets = [
    "In yonder wood where golden paths divide,\nAlas, I mourn’d to tread but one beside.",
    "With wistful breath shall I this tale recite,\nIn times far hence, beyond the span of night.",
    "Yet knowing road to winding road shall yield,\nI fear’d return was fate forever seal’d.",
    "Mine eyes did trace the path till it did turn,\nWhere tangled boughs in secret shade do yearn.",
    "These woods, so fair, so shadow’d and profound,\nYet I must go, for vows do me impound.",
    "Hark, how the ancient hound with weary breath / Doth sound alarm, though rising from his death / Is past his strength; for I can well recall / The frisky youth that did enthrall me all.",
    "Some force there is that doth a wall despise, / Which through the frozen earth doth cause it rise / And hurl the stones that rest upon its height / To lie unmade, exposed to solar light.",
    "For love, this earth doth prove the perfect sphere, / I know not where its joys may be more dear.",
    "The truest path from trouble's maze, I see, / Is by its dark and winding way to flee.",
    "Thus all alone, with none to hear my grief, / I spoke my tale unto the wall, so brief, / A mute stone listener, to my empty sound, / Without a soul, or any ear, profound.",
    "All nature's face did through the darkness peer, / As if to mark his lonely vigil near.",
    "Through crystal frost, like scattered gems they lie, / Each shard a star that twinkles to the eye.",
    "That on the barren glass in stillness clings, / As though in silence winter weaves her rings.",
    "Yet from those eyes no mirrored glance did spring, / For thought was fettered by some weighty thing.",
    "The lamp, askew within his trembling hand, / Cast forth its glow like sun on shadow’d land.",
    "Yet age did weave a veil across his mind, / And thus the thread of memory he could not find.",
    "For time’s cold breath had dimm’d his inward flame, / And left him lost, forgetting why he came."
]

data = {"input": inputs, "target": targets}
dataset = Dataset.from_dict(data)

print(dataset)
print(dataset[0])


def tokenize_function(example):
    input_text = ["transfer style from Robert Frost to Shakespeare: " + text for text in example["input"]]
    model_inputs = tokenizer(input_text, padding="max_length", truncation=True, max_length=128)

    with tokenizer.as_target_tokenizer():
        labels = tokenizer(example["target"], padding="max_length", truncation=True, max_length=128)["input_ids"]

    labels = [[(label if label != tokenizer.pad_token_id else -100) for label in seq] for seq in labels]
    model_inputs["labels"] = labels
    return model_inputs

dataset_split = dataset.train_test_split(test_size=0.2)
tokenized_train = dataset_split["train"].map(tokenize_function, batched=True)
tokenized_eval = dataset_split["test"].map(tokenize_function, batched=True)


bleu = evaluate.load("bleu")
rouge = evaluate.load("rouge")

def compute_metrics(eval_preds):
    preds, labels = eval_preds
    if isinstance(preds, tuple):
        preds = preds[0]

    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)

    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)
    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)

    # Compute BLEU score
    bleu_result = bleu.compute(predictions=decoded_preds, references=[[label] for label in decoded_labels])

    # Compute ROUGE score
    rouge_result = rouge.compute(predictions=decoded_preds, references=decoded_labels)

    result = {
        "bleu": bleu_result["bleu"],
        "rouge1": rouge_result["rouge1"],
        "rouge2": rouge_result["rouge2"],
        "rougeL": rouge_result["rougeL"],
        "rougeLsum": rouge_result["rougeLsum"],
    }

    return result


training_args = Seq2SeqTrainingArguments(
    output_dir="./bart-results",
    learning_rate=2e-5,
    per_device_train_batch_size=4,
    num_train_epochs=10,
    weight_decay=0.01,
    predict_with_generate=True, # Important for generation
    logging_dir='./logs',
    logging_steps=10
)

trainer = Seq2SeqTrainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_train,
    eval_dataset=tokenized_eval,
    tokenizer=tokenizer,
    compute_metrics=compute_metrics,
)

trainer.train()


test_input = """transfer style from Robert Frost to Shakespeare:
Two roads diverged in a yellow wood,
And sorry I could not travel both
And be one traveler, long I stood
And looked down one as far as I could
To where it bent in the undergrowth;
"""

input_ids = tokenizer(test_input, return_tensors="pt").input_ids
outputs = model.generate(input_ids, max_length=512, num_beams=5, early_stopping=True)
print(tokenizer.decode(outputs[0], skip_special_tokens=True))
